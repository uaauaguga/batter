#!/usr/bin/env python
import argparse
import os
import sys
import numpy as np
import torch
from model import TerminatorTagger, MLM
from dataset import tokenize, collate
from torch.nn import functional as F
import logging
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(name)s] %(message)s')
logger = logging.getLogger("tagging terminators")
from collections import defaultdict
import subprocess
from pyfaidx import Fasta
import constants
constants.init(1)


def tagging(batched_tokens, model, nbest=2, temperature=1):
    logits = model(batched_tokens)[0]
    attention_mask = (batched_tokens != constants.tokens_to_id[constants.pad_token]).int()
    tags, scores = model.crf.decode(logits, attention_mask, nbest=nbest)
    tags = tags.cpu().detach().numpy()
    probs = F.softmax(logits/temperature,-1)[:,:,1].cpu().detach().numpy()
    # tags: shape (nbest, batch_size, seq_length)
    # probs: batch size, seq_length
    return tags, probs

def rle(inarray):
        # adapted from https://stackoverflow.com/questions/1066758/find-length-of-sequences-of-identical-values-in-a-numpy-array-run-length-encodi 
        """ run length encoding. Partial credit to R rle function. 
            Multi datatype arrays catered for including non Numpy
            returns: tuple (runlengths, startpositions, values) """
        ia = np.asarray(inarray)                # force numpy
        n = len(ia)
        if n <= 1: 
            return (None, None, None)
        else:
            y = ia[1:] != ia[:-1]               # pairwise unequal (string safe)
            i = np.append(np.where(y), n - 1)   # must include last element posi
            z = np.diff(np.append(-1, i))       # run lengths
            p = np.cumsum(np.append(0, z))[:-1] # positions
            return(z, p, ia[i])

def update_predictions(tags, probs, ivs, min_length = 32):
    # (nbest, batch size, seq length)
    for i in range(tags.shape[0]):
        seq_id, s, e, strand = ivs[i]
        ls, ps, vs = rle(tags[i,:])
        for l, p, v in zip(ls, ps, vs):
            if l is None:
                continue
            if l < min_length:
                continue
            if v == 1:
                if strand == "+":
                    start, end = s+p, s+p+l
                else:
                    start, end = e-p-l + 1, e-p
                if start < 0:
                    logger.warning("some thing goes wrong with {s} {e} {l} {p} {v}")
                    continue
                score = probs[i,p:p+l].mean()
                iv = (seq_id, start, end, strand)
                predictions[iv].append(score)


def clear_cached_predictions(fout):
    ivs = sorted(list(predictions.keys()),key = lambda x:(x[0],x[1]))
    for seq_id, start, end, strand in ivs:
        scores = predictions[(seq_id, start, end, strand)]
        scores = ",".join(np.round(np.array(scores),3).astype(str))
        print(seq_id, start, end, "." , scores, strand, sep="\t", file=fout) 
    fout.flush()
    predictions.clear()


def inference(tagger, batched_tokens, batched_ivs, nbest = 2, device = "cpu", temperature=1.0):
    batched_tokens = collate(batched_tokens)
    batched_tokens = batched_tokens.to(device)
    tags, probs = tagging(batched_tokens, tagger, nbest, temperature) 
    # shape of tags is (nbest, batch_size, seq_length)
    # iterate top k state paths
    for i in range(tags.shape[0]):
        # 1: means here we skip the prepended <cls> token
        update_predictions(tags[i,:,1:], probs[:,1:], batched_ivs)


def main():
    parser = argparse.ArgumentParser(description='tagging terminator in input sequences')
    parser.add_argument('--encoder-config','-ec',default="config/64.8.256.4.json", help="model configuration")
    parser.add_argument('--fasta','-f',type=str,required=True,help="Input genome sequences")
    parser.add_argument('--batch-size','-bs',type=int,default=64,help="Batch size for scanning")
    parser.add_argument('--device','-d',default="cuda:0",choices=["cuda:0","cuda:1","cpu"],help="Device to run the model")
    parser.add_argument('--model','-m',default="model/batter.mdl.pt",type=str,help="Where to load the model parameters")
    parser.add_argument('--window-size','-w',type=int,default=500,help="window size for scanning")
    parser.add_argument('--stride','-s',type=int,default=100,help="stride for scanning")
    parser.add_argument('--output','-o',required=True,type=str,help="path to save plus strand score in bedgraph format")
    parser.add_argument('--reverse-complement','-rc',action="store_true",help="whether consider both strand. only consider forward strand by default")
    parser.add_argument('--tmp-file','-tf',type=str,help="where to save temperatory file")
    parser.add_argument('--keep-tmp','-kt',action="store_true",help="whether keep temporatory file")
    parser.add_argument('--top-k','-k',type=int,default = 10,help="number of state path to return")
    parser.add_argument('--temperature',type=float,default = 1.0,help="temperature for probability calculation")
    parser.add_argument('--verbose','-v',action="store_true", help="whether use verbose output")
    args = parser.parse_args()

    logger.info("Initialize the model ...")
    tagger = TerminatorTagger(MLM(args.encoder_config).encoder)
    logger.info(f"Load model paramters from {args.model} ...")
    state_dict = torch.load(args.model,map_location = args.device)
    tagger.load_state_dict(state_dict)
    tagger = tagger.eval()

    logger.info(f"Will use {args.device} for inference ...")
    tagger.to(args.device)    

    logger.info(f"Load sequences from {args.fasta} ...")
    sequences = Fasta(args.fasta)


    tmp_path = args.tmp_file if args.tmp_file is not None else f"{args.output}.tmp"
    logger.info(f"Intermediate result will be saved to {tmp_path} ...")
    fout = open(tmp_path, "w")

    nbase = 0
    batched_tokens = []
    batched_ivs = []

    
    global predictions
    predictions = defaultdict(list)

    for seq_id  in sequences.keys():
        if args.verbose:
            logger.info(f"processing {seq_id} ...")
        sequence = sequences[seq_id][::]
        p = 0
        while p < len(sequence):
            s = p # start location of this chunk
            e = min(p + args.window_size,len(sequence)) # end location of this chunk
            batched_ivs.append((seq_id, s, e, "+"))
            chunked_sequence = sequence[s:e]
            # we always use RNA alphabet
            tokens = tokenize(str(chunked_sequence).upper().replace("T","U"))
            batched_tokens.append(tokens)
            if args.reverse_complement:
                # if consider the reverse strand
                tokens = tokenize(str(chunked_sequence.reverse.complement).upper().replace("T","U"))
                batched_tokens.append(tokens)
                batched_ivs.append((seq_id, s, e, "-"))
            p += args.stride
            nbase += args.stride
            if nbase%50000 == 0:
                logger.info(f"{int(nbase/1000)} K bases processed .")
            # make a inference when instances accumulate to specified batch size
            if len(batched_tokens) == args.batch_size:
                inference(tagger, batched_tokens, batched_ivs, args.top_k, args.device, args.temperature)
                batched_tokens = [] # clean the batch list
                batched_ivs = []
        clear_cached_predictions(fout)
    # make a inference for the last batch
    if len(batched_tokens) > 0:
        inference(tagger, batched_tokens, batched_ivs, args.top_k, args.device, args.temperature)
        clear_cached_predictions(fout)
    fout.close()


    logger.info("Merge predictions ...")
    logger.info(f"Final results will be saved to {args.output} .")
    subprocess.run(["scripts/pick-local-max.py", "-i", tmp_path, "-o", args.output])
    if not args.keep_tmp:
        logger.info("Remove temporary results ...")
        os.remove(tmp_path)


    logger.info("all done .")
    





if __name__ == "__main__":
    main()
